{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aa285374",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_grad_cam import GradCAM, ScoreCAM, GradCAMPlusPlus, AblationCAM, XGradCAM, EigenCAM, FullGrad\n",
    "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import os, json\n",
    "from glob import glob\n",
    "from tqdm.notebook import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "import torch\n",
    "from torchvision import models, transforms\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import timm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a41c3e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# resize and take the center part of image to what our model expects\n",
    "def get_input_transform():\n",
    "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                    std=[0.229, 0.224, 0.225])       \n",
    "    transf = transforms.Compose([\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        normalize\n",
    "    ])    \n",
    "\n",
    "    return transf\n",
    "\n",
    "def get_input_tensors(img):\n",
    "    transf = get_input_transform()\n",
    "    # unsqeeze converts single image to batch of 1\n",
    "    return transf(img).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7541f09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_fn = \"models/orig_20220627-235055-seresnext50_32x4d-224/last.pth.tar\"\n",
    "im_path = \"data/orig_o\"\n",
    "\n",
    "fl_dict = {}\n",
    "for idx, fl in enumerate(glob(f\"{im_path}/*\")):\n",
    "    bn = os.path.basename(fl)\n",
    "    fl_dict[bn] = idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c69d95e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_fn = \"models/planted_20220627-235814-seresnext50_32x4d-224/last.pth.tar\"\n",
    "im_path = \"data/pl_o\"\n",
    "out_path = \"pl_pl\"\n",
    "\n",
    "fl_dict = {}\n",
    "for idx, fl in enumerate(glob(f\"{im_path}/*\")):\n",
    "    bn = os.path.basename(fl)\n",
    "    fl_dict[bn] = idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "22ce1362",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81297ce74037419f855073cdacfd9cd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7859 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [19]\u001b[0m, in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m test_img \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m255\u001b[39m\n\u001b[1;32m     25\u001b[0m targets \u001b[38;5;241m=\u001b[39m [ClassifierOutputTarget(cat)]\n\u001b[0;32m---> 27\u001b[0m grayscale_cam \u001b[38;5;241m=\u001b[39m \u001b[43mcam\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimg_t\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m grayscale_cam \u001b[38;5;241m=\u001b[39m grayscale_cam[\u001b[38;5;241m0\u001b[39m, :]\n\u001b[1;32m     30\u001b[0m visualization \u001b[38;5;241m=\u001b[39m show_cam_on_image(test_img, grayscale_cam, use_rgb\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniforge3/envs/ml/lib/python3.10/site-packages/pytorch_grad_cam/base_cam.py:184\u001b[0m, in \u001b[0;36mBaseCAM.__call__\u001b[0;34m(self, input_tensor, targets, aug_smooth, eigen_smooth)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m aug_smooth \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    181\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward_augmentation_smoothing(\n\u001b[1;32m    182\u001b[0m         input_tensor, targets, eigen_smooth)\n\u001b[0;32m--> 184\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    185\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meigen_smooth\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/ml/lib/python3.10/site-packages/pytorch_grad_cam/base_cam.py:82\u001b[0m, in \u001b[0;36mBaseCAM.forward\u001b[0;34m(self, input_tensor, targets, eigen_smooth)\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     81\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m([target(output) \u001b[38;5;28;01mfor\u001b[39;00m target, output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(targets, outputs)])\n\u001b[0;32m---> 82\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;66;03m# In most of the saliency attribution papers, the saliency is\u001b[39;00m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;66;03m# computed with a single target layer.\u001b[39;00m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;66;03m# Commonly it is the last convolutional layer.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;66;03m# use all conv layers for example, all Batchnorm layers,\u001b[39;00m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;66;03m# or something else.\u001b[39;00m\n\u001b[1;32m     93\u001b[0m cam_per_layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_cam_per_layer(input_tensor,\n\u001b[1;32m     94\u001b[0m                                            targets,\n\u001b[1;32m     95\u001b[0m                                            eigen_smooth)\n",
      "File \u001b[0;32m~/miniforge3/envs/ml/lib/python3.10/site-packages/torch/_tensor.py:363\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    355\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    356\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    357\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    361\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[1;32m    362\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[0;32m--> 363\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/ml/lib/python3.10/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = timm.create_model(\n",
    "        'seresnext50_32x4d',\n",
    "        num_classes=300,\n",
    "        in_chans=3,\n",
    "        pretrained=True,\n",
    "        checkpoint_path=ckpt_fn)\n",
    "\n",
    "target_layers = model.layer4\n",
    "\n",
    "cam = GradCAM(model=model, target_layers=target_layers)\n",
    "abl = AblationCAM(model=model, target_layers=target_layers)\n",
    "eig = EigenCAM(model=model, target_layers=target_layers)\n",
    "\n",
    "images = glob(f\"{im_path}/**/*.*\", recursive=True)\n",
    "def process(im_fn, fl_dict):\n",
    "    bn = os.path.basename(im_fn)\n",
    "    cat = fl_dict[im_fn.split(\"/\")[-2]]\n",
    "    \n",
    "    img = Image.open(im_fn)\n",
    "    img_t = get_input_tensors(img)\n",
    "    test_img = np.array(img.resize((224, 224)), dtype='float32')\n",
    "    test_img /= 255\n",
    "\n",
    "    targets = [ClassifierOutputTarget(cat)]\n",
    "\n",
    "    grayscale_cam = cam(input_tensor=img_t, targets=targets)\n",
    "    grayscale_cam = grayscale_cam[0, :]\n",
    "\n",
    "    visualization = show_cam_on_image(test_img, grayscale_cam, use_rgb=True)\n",
    "    plt.imshow(visualization)\n",
    "    try:\n",
    "        plt.savefig(f\"outs/{out_path}/gradcam/{bn}\")\n",
    "    except FileNotFoundError:\n",
    "        os.makedirs(f\"outs/{out_path}/gradcam/\")\n",
    "    plt.clf()\n",
    "    \n",
    "    grayscale_abl = abl(input_tensor=img_t, targets=targets)\n",
    "    grayscale_abl = grayscale_abl[0, :]\n",
    "\n",
    "    visualization = show_cam_on_image(test_img, grayscale_abl, use_rgb=True)\n",
    "    plt.imshow(visualization)\n",
    "    plt.savefig(f\"outs/o_o/ablationcam/{bn}\")\n",
    "    plt.clf()\n",
    "    \n",
    "    grayscale_eig = eig(input_tensor=img_t, targets=targets)\n",
    "    grayscale_eig = grayscale_eig[0, :]\n",
    "\n",
    "    visualization = show_cam_on_image(test_img, grayscale_eig, use_rgb=True)\n",
    "    plt.imshow(visualization)\n",
    "    try:\n",
    "        plt.savefig(f\"outs/{out_path}/eigencam/{bn}\")\n",
    "    except FileNotFoundError:\n",
    "        os.makedirs(f\"outs/{out_path}/eigencam/\")\n",
    "    plt.clf()\n",
    "    \n",
    "Parallel(n_jobs=8)(delayed(process)(im_fn, fl_dict) for im_fn in tqdm(images, total=len(list(images))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da50bac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
